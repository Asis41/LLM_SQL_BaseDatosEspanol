{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":61542,"databundleVersionId":7516023,"sourceType":"competition"},{"sourceId":6571530,"sourceType":"datasetVersion","datasetId":3796024},{"sourceId":6847931,"sourceType":"datasetVersion","datasetId":3936750},{"sourceId":148861315,"sourceType":"kernelVersion"}],"dockerImageVersionId":30559,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install mlflow>=2.11.0\n!pip install transformers peft accelerate bitsandbytes datasets -q -U\n!pip install datasets\n!pip install --upgrade git+https://github.com/huggingface/transformers\n!pip install --upgrade torch torchvision\n!pip install --upgrade accelerate","metadata":{"execution":{"iopub.status.busy":"2024-04-11T03:30:47.660589Z","iopub.execute_input":"2024-04-11T03:30:47.661319Z","iopub.status.idle":"2024-04-11T03:32:21.791025Z","shell.execute_reply.started":"2024-04-11T03:30:47.661290Z","shell.execute_reply":"2024-04-11T03:32:21.789641Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Requirement already satisfied: datasets in /opt/conda/lib/python3.10/site-packages (2.18.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from datasets) (3.12.2)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from datasets) (1.23.5)\nRequirement already satisfied: pyarrow>=12.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (15.0.2)\nRequirement already satisfied: pyarrow-hotfix in /opt/conda/lib/python3.10/site-packages (from datasets) (0.6)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.3.7)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets) (2.0.2)\nRequirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (2.31.0)\nRequirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.10/site-packages (from datasets) (4.66.1)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets) (3.3.0)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets) (0.70.15)\nRequirement already satisfied: fsspec[http]<=2024.2.0,>=2023.1.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (2023.9.0)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets) (3.8.4)\nRequirement already satisfied: huggingface-hub>=0.19.4 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.22.2)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from datasets) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from datasets) (6.0)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (23.1.0)\nRequirement already satisfied: charset-normalizer<4.0,>=2.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (3.1.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (6.0.4)\nRequirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.2)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.9.2)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.3)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.19.4->datasets) (4.11.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->datasets) (3.0.9)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (3.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (2023.7.22)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2.8.2)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2023.3)\nRequirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2023.3)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n","output_type":"stream"},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Collecting git+https://github.com/huggingface/transformers\n  Cloning https://github.com/huggingface/transformers to /tmp/pip-req-build-tdntvbxi\n  Running command git clone --filter=blob:none --quiet https://github.com/huggingface/transformers /tmp/pip-req-build-tdntvbxi\n  Resolved https://github.com/huggingface/transformers to commit a5e5c92aea1e99cb84d7342bd63826ca6cd884c4\n  Installing build dependencies ... \u001b[?25ldone\n\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers==4.40.0.dev0) (3.12.2)\nRequirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /opt/conda/lib/python3.10/site-packages (from transformers==4.40.0.dev0) (0.22.2)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers==4.40.0.dev0) (1.23.5)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers==4.40.0.dev0) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers==4.40.0.dev0) (6.0)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers==4.40.0.dev0) (2023.6.3)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers==4.40.0.dev0) (2.31.0)\nRequirement already satisfied: tokenizers<0.19,>=0.14 in /opt/conda/lib/python3.10/site-packages (from transformers==4.40.0.dev0) (0.15.2)\nRequirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers==4.40.0.dev0) (0.4.2)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers==4.40.0.dev0) (4.66.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.40.0.dev0) (2023.9.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.40.0.dev0) (4.11.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers==4.40.0.dev0) (3.0.9)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.40.0.dev0) (3.1.0)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.40.0.dev0) (3.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.40.0.dev0) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.40.0.dev0) (2023.7.22)\n","output_type":"stream"},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Requirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (2.2.2)\nRequirement already satisfied: torchvision in /opt/conda/lib/python3.10/site-packages (0.17.2)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch) (3.12.2)\nRequirement already satisfied: typing-extensions>=4.8.0 in /opt/conda/lib/python3.10/site-packages (from torch) (4.11.0)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch) (3.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch) (3.1.2)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch) (2023.9.0)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /opt/conda/lib/python3.10/site-packages (from torch) (12.1.105)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /opt/conda/lib/python3.10/site-packages (from torch) (12.1.105)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /opt/conda/lib/python3.10/site-packages (from torch) (12.1.105)\nRequirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /opt/conda/lib/python3.10/site-packages (from torch) (8.9.2.26)\nRequirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /opt/conda/lib/python3.10/site-packages (from torch) (12.1.3.1)\nRequirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /opt/conda/lib/python3.10/site-packages (from torch) (11.0.2.54)\nRequirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /opt/conda/lib/python3.10/site-packages (from torch) (10.3.2.106)\nRequirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /opt/conda/lib/python3.10/site-packages (from torch) (11.4.5.107)\nRequirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /opt/conda/lib/python3.10/site-packages (from torch) (12.1.0.106)\nRequirement already satisfied: nvidia-nccl-cu12==2.19.3 in /opt/conda/lib/python3.10/site-packages (from torch) (2.19.3)\nRequirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /opt/conda/lib/python3.10/site-packages (from torch) (12.1.105)\nRequirement already satisfied: triton==2.2.0 in /opt/conda/lib/python3.10/site-packages (from torch) (2.2.0)\nRequirement already satisfied: nvidia-nvjitlink-cu12 in /opt/conda/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch) (12.4.127)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from torchvision) (1.23.5)\nRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/conda/lib/python3.10/site-packages (from torchvision) (9.5.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch) (2.1.3)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch) (1.3.0)\n","output_type":"stream"},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Requirement already satisfied: accelerate in /opt/conda/lib/python3.10/site-packages (0.29.2)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from accelerate) (1.23.5)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from accelerate) (21.3)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate) (5.9.3)\nRequirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from accelerate) (6.0)\nRequirement already satisfied: torch>=1.10.0 in /opt/conda/lib/python3.10/site-packages (from accelerate) (2.2.2)\nRequirement already satisfied: huggingface-hub in /opt/conda/lib/python3.10/site-packages (from accelerate) (0.22.2)\nRequirement already satisfied: safetensors>=0.3.1 in /opt/conda/lib/python3.10/site-packages (from accelerate) (0.4.2)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->accelerate) (3.0.9)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.12.2)\nRequirement already satisfied: typing-extensions>=4.8.0 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (4.11.0)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.1.2)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (2023.9.0)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (12.1.105)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (12.1.105)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (12.1.105)\nRequirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (8.9.2.26)\nRequirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (12.1.3.1)\nRequirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (11.0.2.54)\nRequirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (10.3.2.106)\nRequirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (11.4.5.107)\nRequirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (12.1.0.106)\nRequirement already satisfied: nvidia-nccl-cu12==2.19.3 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (2.19.3)\nRequirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (12.1.105)\nRequirement already satisfied: triton==2.2.0 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (2.2.0)\nRequirement already satisfied: nvidia-nvjitlink-cu12 in /opt/conda/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.10.0->accelerate) (12.4.127)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub->accelerate) (2.31.0)\nRequirement already satisfied: tqdm>=4.42.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub->accelerate) (4.66.1)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.3)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (3.1.0)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (3.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (2023.7.22)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n","output_type":"stream"}]},{"cell_type":"code","source":"import pandas as pd\nfrom datasets import load_dataset\nfrom IPython.display import HTML, display\n\ndataset_name = \"Asis41/SQL_TST\"\ndataset = load_dataset(dataset_name, split=\"train\")\n\n\ndef display_table(dataset_or_sample):\n    # A helper fuction to display a Transformer dataset or single sample contains multi-line string nicely\n    pd.set_option(\"display.max_colwidth\", None)\n    pd.set_option(\"display.width\", None)\n    pd.set_option(\"display.max_rows\", None)\n\n    if isinstance(dataset_or_sample, dict):\n        df = pd.DataFrame(dataset_or_sample, index=[0])\n    else:\n        df = pd.DataFrame(dataset_or_sample)\n\n    html = df.to_html().replace(\"\\\\n\", \"<br>\")\n    styled_html = f\"\"\"<style> .dataframe th, .dataframe tbody td {{ text-align: left; padding-right: 30px; }} </style> {html}\"\"\"\n    display(HTML(styled_html))\n\n\ndisplay_table(dataset.select(range(3)))","metadata":{"execution":{"iopub.status.busy":"2024-04-11T03:32:41.851958Z","iopub.execute_input":"2024-04-11T03:32:41.853038Z","iopub.status.idle":"2024-04-11T03:32:43.293770Z","shell.execute_reply.started":"2024-04-11T03:32:41.852989Z","shell.execute_reply":"2024-04-11T03:32:43.292826Z"},"trusted":true},"execution_count":8,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<style> .dataframe th, .dataframe tbody td { text-align: left; padding-right: 30px; } </style> <table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Question</th>\n      <th>Answer</th>\n      <th>SQL</th>\n      <th>Context</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Dame todas las materias impartidas por el profesor José Roderick Mireles Moreno.</td>\n      <td>SELECT MATERIA FROM escuela WHERE PROFESOR = 'JOSE RODERICK MIRELES MORENO'</td>\n      <td>José Roderick Mireles Moreno imparte las  materias de</td>\n      <td>CREATE TABLE escuela (<br>    CUATRIMESTRE INT,<br>    TURNO VARCHAR(255),<br>    IDCARRERAGRUPO VARCHAR(255),<br>    CARRERA VARCHAR(255),<br>    NO INT,<br>    PROFESOR VARCHAR(255),<br>    MATERIA VARCHAR(255),<br>    escuela VARCHAR(255),<br>    DIA VARCHAR(255),<br>    CANTIDAD  INT,<br>    GRUPO VARCHAR(255),<br>    GRUPO_EN_SISTEMA VARCHAR(255),<br>    AULA VARCHAR(255)<br>)</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Dame la hora y el profesor del grupo A</td>\n      <td>SELECT hora, profesor FROM escuela WHERE GRUPO = 'A'</td>\n      <td>El grupo A1 tiene las clases y el profesor :</td>\n      <td>CREATE TABLE escuela (<br>    CUATRIMESTRE INT,<br>    TURNO VARCHAR(255),<br>    IDCARRERAGRUPO VARCHAR(255),<br>    CARRERA VARCHAR(255),<br>    NO INT,<br>    PROFESOR VARCHAR(255),<br>    MATERIA VARCHAR(255),<br>    escuela VARCHAR(255),<br>    DIA VARCHAR(255),<br>    CANTIDAD  INT,<br>    GRUPO VARCHAR(255),<br>    GRUPO_EN_SISTEMA VARCHAR(255),<br>    AULA VARCHAR(255)<br>)</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Dime todos los profesores que imparten la materia Mecatrónica</td>\n      <td>SELECT PROFESOR FROM escuela WHERE MATERIA = 'MECATRÓNICA'</td>\n      <td>La materia Mecatrónica es impartida por los maestros</td>\n      <td>CREATE TABLE escuela (<br>    CUATRIMESTRE INT,<br>    TURNO VARCHAR(255),<br>    IDCARRERAGRUPO VARCHAR(255),<br>    CARRERA VARCHAR(255),<br>    NO INT,<br>    PROFESOR VARCHAR(255),<br>    MATERIA VARCHAR(255),<br>    escuela VARCHAR(255),<br>    DIA VARCHAR(255),<br>    CANTIDAD  INT,<br>    GRUPO VARCHAR(255),<br>    GRUPO_EN_SISTEMA VARCHAR(255),<br>    AULA VARCHAR(255)<br>)</td>\n    </tr>\n  </tbody>\n</table>"},"metadata":{}}]},{"cell_type":"code","source":"split_dataset = dataset.train_test_split(test_size=0.2, seed=42)\ntrain_dataset = split_dataset[\"train\"]\ntest_dataset = split_dataset[\"test\"]\n\nprint(f\"Training dataset contains {len(train_dataset)} text-to-SQL pairs\")\nprint(f\"Test dataset contains {len(test_dataset)} text-to-SQL pairs\")","metadata":{"execution":{"iopub.status.busy":"2024-04-11T03:39:58.250972Z","iopub.execute_input":"2024-04-11T03:39:58.251808Z","iopub.status.idle":"2024-04-11T03:39:58.264254Z","shell.execute_reply.started":"2024-04-11T03:39:58.251766Z","shell.execute_reply":"2024-04-11T03:39:58.263271Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"Training dataset contains 64 text-to-SQL pairs\nTest dataset contains 17 text-to-SQL pairs\n","output_type":"stream"}]},{"cell_type":"code","source":"PROMPT_TEMPLATE = \"\"\"You are a powerful text-to-SQL model. Given the SQL tables and natural language question, your job is to write SQL query that answers the question.\n\n### Table:\n{context}\n\n### Question:\n{question}\n\n### Response:\n{output}\"\"\"\n\n\ndef apply_prompt_template(row):\n    prompt = PROMPT_TEMPLATE.format(\n        question=row[\"Question\"],\n        context=row[\"Context\"],\n        output=row[\"Answer\"],\n    )\n    return {\"prompt\": prompt}\n\n\ntrain_dataset = train_dataset.map(apply_prompt_template)\ndisplay_table(train_dataset.select(range(1)))","metadata":{"execution":{"iopub.status.busy":"2024-04-11T03:39:58.572397Z","iopub.execute_input":"2024-04-11T03:39:58.573225Z","iopub.status.idle":"2024-04-11T03:39:58.588511Z","shell.execute_reply.started":"2024-04-11T03:39:58.573193Z","shell.execute_reply":"2024-04-11T03:39:58.587701Z"},"trusted":true},"execution_count":10,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<style> .dataframe th, .dataframe tbody td { text-align: left; padding-right: 30px; } </style> <table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Question</th>\n      <th>Answer</th>\n      <th>SQL</th>\n      <th>Context</th>\n      <th>prompt</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Dame la hora a la que  materia Investigación en Ciencia y Tecnología se imparte en el grupo B</td>\n      <td>SELECT hora FROM escuela WHERE MATERIA = 'INVESTIGACIÓN EN CIENCIA Y TECNOLOGÍA' AND GRUPO = 'B'</td>\n      <td>La materia Investigación en Ciencia y Tecnología se imparte en el grupo B a las  :</td>\n      <td>CREATE TABLE escuela (<br>    CUATRIMESTRE INT,<br>    TURNO VARCHAR(255),<br>    IDCARRERAGRUPO VARCHAR(255),<br>    CARRERA VARCHAR(255),<br>    NO INT,<br>    PROFESOR VARCHAR(255),<br>    MATERIA VARCHAR(255),<br>    escuela VARCHAR(255),<br>    DIA VARCHAR(255),<br>    CANTIDAD  INT,<br>    GRUPO VARCHAR(255),<br>    GRUPO_EN_SISTEMA VARCHAR(255),<br>    AULA VARCHAR(255)<br>)</td>\n      <td>You are a powerful text-to-SQL model. Given the SQL tables and natural language question, your job is to write SQL query that answers the question.<br><br>### Table:<br>CREATE TABLE escuela (<br>    CUATRIMESTRE INT,<br>    TURNO VARCHAR(255),<br>    IDCARRERAGRUPO VARCHAR(255),<br>    CARRERA VARCHAR(255),<br>    NO INT,<br>    PROFESOR VARCHAR(255),<br>    MATERIA VARCHAR(255),<br>    escuela VARCHAR(255),<br>    DIA VARCHAR(255),<br>    CANTIDAD  INT,<br>    GRUPO VARCHAR(255),<br>    GRUPO_EN_SISTEMA VARCHAR(255),<br>    AULA VARCHAR(255)<br>)<br><br>### Question:<br>Dame la hora a la que  materia Investigación en Ciencia y Tecnología se imparte en el grupo B<br><br>### Response:<br>SELECT hora FROM escuela WHERE MATERIA = 'INVESTIGACIÓN EN CIENCIA Y TECNOLOGÍA' AND GRUPO = 'B'</td>\n    </tr>\n  </tbody>\n</table>"},"metadata":{}}]},{"cell_type":"code","source":"from transformers import AutoTokenizer\nimport torch\nfrom transformers import AutoModelForCausalLM, BitsAndBytesConfig\n\nbase_model_id = \"mistralai/Mistral-7B-v0.1\"\n\n# You can use a different max length if your custom dataset has shorter/longer input sequences.\nMAX_LENGTH = 256\n\nquantization_config = BitsAndBytesConfig(\n    # Load the model with 4-bit quantization\n    load_in_4bit=True,\n    # Use double quantization\n    bnb_4bit_use_double_quant=True,\n    # Use 4-bit Normal Float for storing the base model weights in GPU memory\n    bnb_4bit_quant_type=\"nf4\",\n    # De-quantize the weights to 16-bit (Brain) float before the forward/backward pass\n    bnb_4bit_compute_dtype=torch.bfloat16,\n)\n\nmodel = AutoModelForCausalLM.from_pretrained(base_model_id, quantization_config=quantization_config)\n\n\ntokenizer = AutoTokenizer.from_pretrained(\n    base_model_id,\n    model_max_length=MAX_LENGTH,\n    padding_side=\"left\",\n    add_eos_token=True,\n)\n\n  \n\n\ndef tokenize_and_pad_to_fixed_length(sample):\n    result = tokenizer(\n        sample[\"prompt\"],\n        truncation=True,\n        max_length=MAX_LENGTH,\n        padding=\"max_length\",\n    )\n    result[\"labels\"] = result[\"input_ids\"].copy()\n    return result\n\nif tokenizer.pad_token is None:\n    tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n    model.resize_token_embeddings(len(tokenizer))  \n\ntokenized_train_dataset = train_dataset.map(tokenize_and_pad_to_fixed_length)\n\nassert all(len(x[\"input_ids\"]) == MAX_LENGTH for x in tokenized_train_dataset)\n\ndisplay_table(tokenized_train_dataset.select(range(1)))","metadata":{"execution":{"iopub.status.busy":"2024-04-11T03:40:01.156179Z","iopub.execute_input":"2024-04-11T03:40:01.156661Z","iopub.status.idle":"2024-04-11T03:40:19.022523Z","shell.execute_reply.started":"2024-04-11T03:40:01.156630Z","shell.execute_reply":"2024-04-11T03:40:19.021506Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stderr","text":"`low_cpu_mem_usage` was None, now set to True since model is quantized.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dcce719b58374041a4cd623c271e0492"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<style> .dataframe th, .dataframe tbody td { text-align: left; padding-right: 30px; } </style> <table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Question</th>\n      <th>Answer</th>\n      <th>SQL</th>\n      <th>Context</th>\n      <th>prompt</th>\n      <th>input_ids</th>\n      <th>attention_mask</th>\n      <th>labels</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Dame la hora a la que  materia Investigación en Ciencia y Tecnología se imparte en el grupo B</td>\n      <td>SELECT hora FROM escuela WHERE MATERIA = 'INVESTIGACIÓN EN CIENCIA Y TECNOLOGÍA' AND GRUPO = 'B'</td>\n      <td>La materia Investigación en Ciencia y Tecnología se imparte en el grupo B a las  :</td>\n      <td>CREATE TABLE escuela (<br>    CUATRIMESTRE INT,<br>    TURNO VARCHAR(255),<br>    IDCARRERAGRUPO VARCHAR(255),<br>    CARRERA VARCHAR(255),<br>    NO INT,<br>    PROFESOR VARCHAR(255),<br>    MATERIA VARCHAR(255),<br>    escuela VARCHAR(255),<br>    DIA VARCHAR(255),<br>    CANTIDAD  INT,<br>    GRUPO VARCHAR(255),<br>    GRUPO_EN_SISTEMA VARCHAR(255),<br>    AULA VARCHAR(255)<br>)</td>\n      <td>You are a powerful text-to-SQL model. Given the SQL tables and natural language question, your job is to write SQL query that answers the question.<br><br>### Table:<br>CREATE TABLE escuela (<br>    CUATRIMESTRE INT,<br>    TURNO VARCHAR(255),<br>    IDCARRERAGRUPO VARCHAR(255),<br>    CARRERA VARCHAR(255),<br>    NO INT,<br>    PROFESOR VARCHAR(255),<br>    MATERIA VARCHAR(255),<br>    escuela VARCHAR(255),<br>    DIA VARCHAR(255),<br>    CANTIDAD  INT,<br>    GRUPO VARCHAR(255),<br>    GRUPO_EN_SISTEMA VARCHAR(255),<br>    AULA VARCHAR(255)<br>)<br><br>### Question:<br>Dame la hora a la que  materia Investigación en Ciencia y Tecnología se imparte en el grupo B<br><br>### Response:<br>SELECT hora FROM escuela WHERE MATERIA = 'INVESTIGACIÓN EN CIENCIA Y TECNOLOGÍA' AND GRUPO = 'B'</td>\n      <td>[1, 995, 460, 264, 6787, 2245, 28733, 532, 28733, 9295, 2229, 28723, 12628, 272, 13208, 9882, 304, 4229, 3842, 2996, 28725, 574, 2389, 349, 298, 3324, 13208, 5709, 369, 11194, 272, 2996, 28723, 13, 13, 27332, 7582, 28747, 13, 21005, 23740, 3431, 13482, 325, 13, 2287, 334, 28779, 962, 8671, 1574, 920, 896, 13289, 28725, 13, 2287, 320, 1990, 4032, 27913, 12064, 28732, 28750, 28782, 28782, 557, 13, 2287, 4519, 28743, 1087, 28754, 725, 2377, 28754, 28779, 3402, 27913, 12064, 28732, 28750, 28782, 28782, 557, 13, 2287, 334, 1087, 28754, 725, 28741, 27913, 12064, 28732, 28750, 28782, 28782, 557, 13, ...]</td>\n      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...]</td>\n      <td>[1, 995, 460, 264, 6787, 2245, 28733, 532, 28733, 9295, 2229, 28723, 12628, 272, 13208, 9882, 304, 4229, 3842, 2996, 28725, 574, 2389, 349, 298, 3324, 13208, 5709, 369, 11194, 272, 2996, 28723, 13, 13, 27332, 7582, 28747, 13, 21005, 23740, 3431, 13482, 325, 13, 2287, 334, 28779, 962, 8671, 1574, 920, 896, 13289, 28725, 13, 2287, 320, 1990, 4032, 27913, 12064, 28732, 28750, 28782, 28782, 557, 13, 2287, 4519, 28743, 1087, 28754, 725, 2377, 28754, 28779, 3402, 27913, 12064, 28732, 28750, 28782, 28782, 557, 13, 2287, 334, 1087, 28754, 725, 28741, 27913, 12064, 28732, 28750, 28782, 28782, 557, 13, ...]</td>\n    </tr>\n  </tbody>\n</table>"},"metadata":{}}]},{"cell_type":"code","source":"import transformers\n\ntokenizer = AutoTokenizer.from_pretrained(base_model_id)\npipeline = transformers.pipeline(model=model, tokenizer=tokenizer, task=\"text-generation\")\n\nif tokenizer.pad_token is None:\n    tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n    model.resize_token_embeddings(len(tokenizer)) \n\nsample = test_dataset[1]\nprompt = PROMPT_TEMPLATE.format(\n    context=sample[\"Context\"], question=sample[\"Question\"], output=\"\"\n)  # Leave the answer part blank\n\nwith torch.no_grad():\n    response = pipeline(prompt, max_new_tokens=256, repetition_penalty=1.15, return_full_text=False)\n\ndisplay_table({\"prompt\": prompt, \"generated_query\": response[0][\"generated_text\"]})","metadata":{"execution":{"iopub.status.busy":"2024-04-11T03:43:19.369494Z","iopub.execute_input":"2024-04-11T03:43:19.369891Z","iopub.status.idle":"2024-04-11T03:43:22.437590Z","shell.execute_reply.started":"2024-04-11T03:43:19.369860Z","shell.execute_reply":"2024-04-11T03:43:22.436795Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<style> .dataframe th, .dataframe tbody td { text-align: left; padding-right: 30px; } </style> <table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>prompt</th>\n      <th>generated_query</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>You are a powerful text-to-SQL model. Given the SQL tables and natural language question, your job is to write SQL query that answers the question.<br><br>### Table:<br>CREATE TABLE escuela (<br>    CUATRIMESTRE INT,<br>    TURNO VARCHAR(255),<br>    IDCARRERAGRUPO VARCHAR(255),<br>    CARRERA VARCHAR(255),<br>    NO INT,<br>    PROFESOR VARCHAR(255),<br>    MATERIA VARCHAR(255),<br>    escuela VARCHAR(255),<br>    DIA VARCHAR(255),<br>    CANTIDAD  INT,<br>    GRUPO VARCHAR(255),<br>    GRUPO_EN_SISTEMA VARCHAR(255),<br>    AULA VARCHAR(255)<br>)<br><br>### Question:<br>¿Qué materias se imparten los sábados en el aula del auditorio?<br><br>### Response:<br></td>\n      <td>SELECT * FROM escuela WHERE DIA = 'SAB' AND AULA = 'AUDI'</td>\n    </tr>\n  </tbody>\n</table>"},"metadata":{}}]},{"cell_type":"code","source":"from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n\n# Enabling gradient checkpointing, to make the training further efficient\nmodel.gradient_checkpointing_enable()\n# Set up the model for quantization-aware training e.g. casting layers, parameter freezing, etc.\nmodel = prepare_model_for_kbit_training(model)\n\npeft_config = LoraConfig(\n    task_type=\"CAUSAL_LM\",\n    # This is the rank of the decomposed matrices A and B to be learned during fine-tuning. A smaller number will save more GPU memory but might result in worse performance.\n    r=32,\n    # This is the coefficient for the learned ΔW factor, so the larger number will typically result in a larger behavior change after fine-tuning.\n    lora_alpha=64,\n    # Drop out ratio for the layers in LoRA adaptors A and B.\n    lora_dropout=0.1,\n    # We fine-tune all linear layers in the model. It might sound a bit large, but the trainable adapter size is still only **1.16%** of the whole model.\n    target_modules=[\n        \"q_proj\",\n        \"k_proj\",\n        \"v_proj\",\n        \"o_proj\",\n        \"gate_proj\",\n        \"up_proj\",\n        \"down_proj\",\n        \"lm_head\",\n    ],\n    # Bias parameters to train. 'none' is recommended to keep the original model performing equally when turning off the adapter.\n    bias=\"none\",\n)\n\npeft_model = get_peft_model(model, peft_config)","metadata":{"execution":{"iopub.status.busy":"2024-04-11T03:43:24.730974Z","iopub.execute_input":"2024-04-11T03:43:24.731487Z","iopub.status.idle":"2024-04-11T03:43:26.076119Z","shell.execute_reply.started":"2024-04-11T03:43:24.731449Z","shell.execute_reply":"2024-04-11T03:43:26.075273Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"from datetime import datetime\n\nimport transformers\nfrom transformers import TrainingArguments\n\n#import mlflow\n\n# Comment-out this line if you are running the tutorial on Databricks\n#mlflow.set_experiment(\"MLflow PEFT Tutorial\")\n\ntraining_args = TrainingArguments(\n    # Set this to mlflow for logging your training\n    #report_to=\"mlflow\",\n    # Name the MLflow run\n    run_name=f\"Mistral-7B-SQL-QLoRA-{datetime.now().strftime('%Y-%m-%d-%H-%M-%s')}\",\n    # Replace with your output destination\n    output_dir=\"TEST_1\",\n    # For the following arguments, refer to https://huggingface.co/docs/transformers/main_classes/trainer\n    per_device_train_batch_size=2,\n    gradient_accumulation_steps=4,\n    gradient_checkpointing=True,\n    optim=\"paged_adamw_8bit\",\n    bf16=False,\n    learning_rate=2e-5,\n    lr_scheduler_type=\"constant\",\n    max_steps=100,\n    save_steps=100,\n    logging_steps=100,\n    warmup_steps=5,\n    # https://discuss.huggingface.co/t/training-llama-with-lora-on-multiple-gpus-may-exist-bug/47005/3\n    ddp_find_unused_parameters=False,\n)\n\ntrainer = transformers.Trainer(\n    model=peft_model,\n    train_dataset=tokenized_train_dataset,\n    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False),\n    args=training_args,\n)\n\n# use_cache=True is incompatible with gradient checkpointing.\npeft_model.config.use_cache = False","metadata":{"execution":{"iopub.status.busy":"2024-04-11T03:43:41.060311Z","iopub.execute_input":"2024-04-11T03:43:41.061012Z","iopub.status.idle":"2024-04-11T03:43:41.116222Z","shell.execute_reply.started":"2024-04-11T03:43:41.060977Z","shell.execute_reply":"2024-04-11T03:43:41.115256Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stderr","text":"max_steps is given, it will override any value given in num_train_epochs\n","output_type":"stream"}]},{"cell_type":"code","source":"trainer.train()","metadata":{"execution":{"iopub.status.busy":"2024-04-11T03:43:43.057085Z","iopub.execute_input":"2024-04-11T03:43:43.057509Z","iopub.status.idle":"2024-04-11T04:20:42.817011Z","shell.execute_reply.started":"2024-04-11T03:43:43.057472Z","shell.execute_reply":"2024-04-11T04:20:42.815654Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"  ········································\n"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"wandb version 0.16.6 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.15.9"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20240411_034514-gayox8bw</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/tst_codellama/huggingface/runs/gayox8bw' target=\"_blank\">Mistral-7B-SQL-QLoRA-2024-04-11-03-43-1712807021</a></strong> to <a href='https://wandb.ai/tst_codellama/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/tst_codellama/huggingface' target=\"_blank\">https://wandb.ai/tst_codellama/huggingface</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/tst_codellama/huggingface/runs/gayox8bw' target=\"_blank\">https://wandb.ai/tst_codellama/huggingface/runs/gayox8bw</a>"},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [100/100 34:35, Epoch 12/13]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>100</td>\n      <td>0.160900</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/peft/utils/save_and_load.py:139: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n","output_type":"stream"},{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=100, training_loss=0.16088205337524414, metrics={'train_runtime': 2219.1187, 'train_samples_per_second': 0.361, 'train_steps_per_second': 0.045, 'total_flos': 8842082766028800.0, 'train_loss': 0.16088205337524414, 'epoch': 12.5})"},"metadata":{}}]},{"cell_type":"code","source":"from huggingface_hub import login\nlogin()","metadata":{"execution":{"iopub.status.busy":"2024-04-11T04:51:38.094275Z","iopub.execute_input":"2024-04-11T04:51:38.095211Z","iopub.status.idle":"2024-04-11T04:51:38.124411Z","shell.execute_reply.started":"2024-04-11T04:51:38.095174Z","shell.execute_reply":"2024-04-11T04:51:38.123492Z"},"trusted":true},"execution_count":17,"outputs":[{"output_type":"display_data","data":{"text/plain":"VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"232e6fd86cd948499f3cba2056b996cb"}},"metadata":{}}]},{"cell_type":"code","source":"# push to the hub\nmodel.push_to_hub(\"somosnlp/LLM_SQL_BaseDatosEspanol\")# ,config=training_args)","metadata":{"execution":{"iopub.status.busy":"2024-04-11T04:52:10.259258Z","iopub.execute_input":"2024-04-11T04:52:10.260196Z","iopub.status.idle":"2024-04-11T04:54:41.583661Z","shell.execute_reply.started":"2024-04-11T04:52:10.260160Z","shell.execute_reply":"2024-04-11T04:54:41.581894Z"},"trusted":true},"execution_count":18,"outputs":[{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/28.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a825c08789164aad9a5e8f36dccfa1d9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/4.99G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7909bd9a695c484c90da651db7e3e0d9"}},"metadata":{}},{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"CommitInfo(commit_url='https://huggingface.co/somosnlp/LLM_SQL_BaseDatosEspanol/commit/91874e83b97af64f30ecb40db3a5e76923b87aad', commit_message='Upload MistralForCausalLM', commit_description='', oid='91874e83b97af64f30ecb40db3a5e76923b87aad', pr_url=None, pr_revision=None, pr_num=None)"},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}